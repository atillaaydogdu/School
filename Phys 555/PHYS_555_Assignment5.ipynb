{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tKczI139PJhV"
   },
   "source": [
    "# Database and Machine Learning Basics\n",
    "\n",
    "**PHYS-555 Winter 2020 - Assignment #5**\n",
    "\n",
    "\n",
    "Copy this notebook, edit and format it as you wish, as long as we can understand you answered the questions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p0z6ZyNjOb51"
   },
   "source": [
    "## Knowledge Part (50%)\n",
    "\n",
    "You can answer the questions with a notebook cell below each question.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Iw60nd4rO3lu"
   },
   "source": [
    "1. You are about to run simulations on an HPC cluster which will produce one million outputs. Each output contains:\n",
    "  - a description of the output: date of simulation, number of parameters M, and the value for each parameters (the metadata)\n",
    "  - an array of 100,000 elements representaing the generated synthetic data set for this set of parameters.\n",
    "\n",
    "  Describe briefly how you would store your outputs in such a way you could explore, search and share the results efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_IrUQcdMRQ1D"
   },
   "source": [
    "<font color='green'>For the scenario above, the best way to store the data would be in an SQL database format where the metadata are stored in seperate columns and the array of 100000 elements is stored in its own column. This would allow one to use database queries to search for specific combinations of the metadata and obtain the corresponding data from these metadata combinations.  </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y-kvlRrLRUsX"
   },
   "source": [
    "2. Give a situation where you would you use a NoSQL database document \n",
    "store rather than a traditional single relational (SQL) database.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o-dpp2XlKu-G"
   },
   "source": [
    "<font color='green'>A NoSQL database is used to store data that is not able to be converted to tabular format. An example of this is when different objects stored in the database have different attributes. For example, suppose one creates a data base of plants and animals. While the attribute \"tail-length\" may be appropriate for something like a cat, it is not an attribute of a apple tree. In such a scenario where the attributes differ greatly, a NoSQL database should be used. The specific NoSQL database used here might be \"Document database\" where a given key (i.e \"apple tree\") links to a complex data structure known as a document which itself has many different key-value pairs.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vth8lN1FR9UX"
   },
   "source": [
    "3. What is the main issue you have to tackle in a distributed database? Explain briefly the difference between a CA and a CP database and when you could choose one vs the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O0k3OkowKxQ7"
   },
   "source": [
    "<font color='green'>The main issue that needs to be tackled when creating a distributed database is the tradeoff between consistency (every read receives the most recent write or an error), availability (every read always obtains a non-error response but without the guarentee that it contains the most recent write), and partition tolerance (the system continues to operate despite some messages being dropped by the network between nodes). A CA database has consistency and availability and a CP database has consistency and partition tolerance. A CA database should be used when users don't necessarily require the most recent write; for example: stock prices are always changing but a delay of a few hours is likely not important for the average consumer. On the other hand, a stock brocker who makes quick trades on the market requires the most up-to-date information and would prefer the database to be CP. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ML9zsKkySihR"
   },
   "source": [
    "4. You are collecting data from geographically distributed sensors in the ocean. You want to keep GPS coordinates, depth for each sensor, and the pressure, temperature every day. Describe a simple relational data model to store the data (i.e. what are the tables you are creating), so that you can query your data with SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "My2xhOd1KyiB"
   },
   "source": [
    "<font color='green'>Suppose the GPS coordinates and depth for each sensor **change** every day. In such a such a scenario, an additional identification number would need to be associated with each sensor. The data model would be as such: a primary table would contain an index column of identification numbers and a data column where each entry itself contains a table. Each table would include 5 columns: timestamps, the GPS coordinates, the depth for each sensor, the pressure, and the temperature. Now suppose the GPS coordinates and depth for each sensor **remain the same** every day. One might then structure the database as follows: the primary table would contain 1) two index columns: GPS coordinates and depth and 2) one data column where each entry us a table. Each table would include 3 columns: timestamps, pressures, and temperatures. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rRu9_QJLTDmz"
   },
   "source": [
    "5. Where is the query executed when:\n",
    "  - a) you are querying from a python script running on your laptop a table kept in \n",
    "Google BigQuery\n",
    "  - b) you are querying a pandas dataframe from a jupyter session launched from your laptop\n",
    "  - c) you are querying a BigQuery table in a Google Colab session accessed from your laptop browser\n",
    "  - d) you have launched a job from the Compute Canada graham cluster which will query tables in a PostgresQL database on the cedar cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4FFzv9t2K0q_"
   },
   "source": [
    "<font color='green'>a) wherever the table is stored on BigQuery b) on your laptop c) wherever the table is stored on BigQuery d) the cedar cluster</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "voWIBnkHTfxK"
   },
   "source": [
    "6. Why do we use three sets (training, validation and test sets) in machine learning training procedures? In other words, what are the differences between validation and test sets? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6u6cjdqlK110"
   },
   "source": [
    "<font color='green'>The validation set is used to refine the hyperparameters used when training the machine learning model. The test set should only be used at the very end (once the hyperparameters have been decided) to test the strength of the model. Once the test set has been used, the hyperparameters (ideally) should not be adjusted anymore so that the model does not fit to the test set in any way.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BGrfWMWNTpcM"
   },
   "source": [
    "7. What are the relationship between maximum likelihood estimation of a linear model and linear model regression used in machine learning software such as scikit-learn?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TfUH2iKPK3HV"
   },
   "source": [
    "<font color='green'>The relationship is as follows. Suppose the expectation for each measurement is $$E[y_i] = w^T x_i + b$$ and the variance is constant $$V[y_i]=\\sigma^2 $$ and further more that the $y_i$'s are Gaussian distributed. Then the likelihood function is given by $$L(w) = \\prod_i \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\left(-\\frac{(y_i-(w^Tx_i +b))^2}{2 \\sigma^2} \\right)$$ $$\\ln L(w) = \\text{const}-\\frac{1}{2 \\sigma^2}\\sum_i (y_i-(w^Tx_i+b))^2 $$ and thus to maximize the likehood function we need to minimize $$\\sum_i (y_i-(w^Tx_i+b))^2$$ which is precisely the aim of a linear regression model. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I7e1jMtMT0nD"
   },
   "source": [
    "8. Why do we normalize (or scale) our data before the training step? Explore the [scikit-learn documentation](https://scikit-learn.org) and find some normalization functions. Should we normalize a data  set for the PCA method? Name a method (or methods) that does not need a normalization step for the training procedure.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rxf95QZ0Ll3z"
   },
   "source": [
    "<font color='green'>For most machine learning, data needs to be scaled (to be of the same magnitude) before each training step. This requirement is obvious when we consider the following case. Suppose we are using a linear regression and we want to miminize $$\\sum_i (y_i-(w^Tx_i+b))^2$$ If one of the features in each $x_i$ is of the scale of $10^9$ and the other is of the scale $10^{-9}$, then the weights $w$ will be adjusted so that the $10^9$ feature and $w_j$ (one of the weights) is as small as possible, but the $10^{-9}$ feature will bear little importance since its product with $w_{j'}$ (another one of the weights) will be small anyways. An exception for feature scaling is decision trees/random forests since they don't work by mimimizing a typical cost function.  </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1oHhvhWeT9Q2"
   },
   "source": [
    "9. Explain very briefly the overfitting problem in machine learning. Give at least one solution to mitigate overfitting. From a Bayesian perspective, how would the Bayes rule relate to overfitting mitigation?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WyVFaAtgLn2G"
   },
   "source": [
    "<font color='green'>Overfitting in machine learning occurs when a model (typically with complex learning capabilities) minimizes some error function on a training set significantly but the error function starts to grow with respect to the test/validation set. There are many ways to prevent overfitting: some basic ones include decreasing the complexity of the model, $l_1$ regularization, and early stopping.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x6WpRNisUE8Y"
   },
   "source": [
    "10. Explain how to obtain a cumulative explained variance plot (see the PCA lecture). Explain how PCA can be used to reduce noise in images (image denoising)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ptyTUs-qLpMm"
   },
   "source": [
    "<font color='green'>Getting the cumulative explained variance plot is a non-trivial mathematical procedure. Suppose we have an $n\\times m $ matrix $X$ where $n$ is the number of data points and $m$ is the number of features. Suppose we have an $m$ dimensional unit vector $w$ pointing in some arbiratry direction in our feature space. The product $Xw$ then yields a vector where each entry contains a linear combination of all features for a given data point. An optimal direction $w$ is one that produces the maximum variance for all data points along this direction: if all entries have zero mean then the variance is given by $$V=\\frac{1}{n-1} (Xw)^T Xw = \\frac{1}{n-1} w^T X^T X w =  w^T C w $$ where $C$ is the covariance matrix. Our goal is thus to maximize $w^T C w $ subject to $w^Tw = 1$, which can be cast as the following Lagrange multiplier problem:$$ \\frac{\\partial}{\\partial w_i} \\left(w^T C w - \\lambda (w^T w-1) \\right)=0 $$. Solving this produces $$Cw = \\lambda w $$ Noting that $w^T C w = w^T \\lambda w = \\lambda w^T w = \\lambda$ we see that $w^T C w$ is maximized by choosing the largest value in the set of eigenvectors $\\{\\lambda_i\\}$ which corresponds to a specific $w_i$:  the principal component. Furthermore, this $\\lambda_i$ is the variance in this direction of the parameter space. Subsequently smaller values of $\\lambda_i$ (and their corresponding directions $w_i$) lead to other directions in parameter space that have smaller variance. Finally, we can show that variance is conserved. A mathematical theorem says the trace of a square matrix is equal to the sum of its eigenvalues, i.e.  $$\\text{Tr}(C) = \\sum_i \\lambda_i $$ The trace of $C$ is simply the sum of the variances of each of the parameters (variances lie on the diagonal of a covariance matrix) and the sum of $\\lambda$'s is simply the sum of all the different variances of each principle component $w$. **The cumulative explained variance plot is simply a bar plot where each value of $\\lambda_i$ is plotted in decreasing order. In sci-kit learn, one can use \"from sklearn.decomposition import PCA\" and then \"pca=PCA()\" followed by \"XPCA = pca.fit_transform(X)\" followed by \"pca.explaned_variance_ratio_\" to get the values of the $\\lambda_i$'s (the variance along each principal component)** </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZQzRaqCSUlkR"
   },
   "source": [
    "## Practice Part (50%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H7MyG8AqUpla"
   },
   "source": [
    "The point of this exercise will be to:\n",
    "- retrieve data from BigQuery with selection on the fly\n",
    "- practice your SQL ninja skills\n",
    "- understand data preparation\n",
    "- try kNN and linear models for regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "45b_akrbW7Hh"
   },
   "source": [
    "You may want to practice with BigQuery [quick tutorial](https://colab.research.google.com/notebooks/bigquery.ipynb) if you did not get a chance to do so with the quick tutorial.\n",
    "\n",
    "* first login to Google, and go to the Google Compute Engine [Cloud Resource Manager](https://console.cloud.google.com/cloud-resource-manager) to Create a Cloud Platform project if you do not already have one.\n",
    "* then authenticate the colab notebook to allow access to BigQuery just like in the tutorial linked above.\n",
    "\n",
    "Try to make a project and a dataset. You should have access to the dataset `assign5` associated with the `phys-555-2020` project. You should be able to query both:\n",
    "- the target labels table ID: `phys-555-2020.assign5.targets`\n",
    "- the input features table ID: `phys-555-2020.assign5.inputs`\n",
    "\n",
    "For questions 1-3, expect to spend time reading SQL / BigQuery documentation. For questions 4-7, expect scikit-learn reading. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fVOWdLQkVlhd"
   },
   "source": [
    "1. Explore either with python script or using the BigQuery [console](https://console.cloud.google.com/bigquery) interface the target and labels datasets. How many fields in each of the table? How many NaN does the input table have? Write your SQL queries or your python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "an6hd3cxD3jV"
   },
   "outputs": [
    {
     "ename": "DefaultCredentialsError",
     "evalue": "Could not automatically determine credentials. Please set GOOGLE_APPLICATION_CREDENTIALS or explicitly create credentials and re-run the application. For more information, please see https://cloud.google.com/docs/authentication/getting-started",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDefaultCredentialsError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-731e48df9caf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcloud\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbigquery\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mclient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbigquery\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mClient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\google\\cloud\\bigquery\\client.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, project, credentials, _http, location, default_query_job_config, client_info, client_options)\u001b[0m\n\u001b[0;32m    175\u001b[0m     ):\n\u001b[0;32m    176\u001b[0m         super(Client, self).__init__(\n\u001b[1;32m--> 177\u001b[1;33m             \u001b[0mproject\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mproject\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcredentials\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcredentials\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_http\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_http\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    178\u001b[0m         )\n\u001b[0;32m    179\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\google\\cloud\\client.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, project, credentials, _http)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproject\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcredentials\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_http\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 226\u001b[1;33m         \u001b[0m_ClientProjectMixin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproject\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mproject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    227\u001b[0m         \u001b[0mClient\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcredentials\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcredentials\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_http\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_http\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\google\\cloud\\client.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, project)\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproject\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 178\u001b[1;33m         \u001b[0mproject\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_determine_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    179\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mproject\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m             raise EnvironmentError(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\google\\cloud\\client.py\u001b[0m in \u001b[0;36m_determine_default\u001b[1;34m(project)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_determine_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m         \u001b[1;34m\"\"\"Helper:  use default project detection.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_determine_default_project\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\google\\cloud\\_helpers.py\u001b[0m in \u001b[0;36m_determine_default_project\u001b[1;34m(project)\u001b[0m\n\u001b[0;32m    184\u001b[0m     \"\"\"\n\u001b[0;32m    185\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mproject\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 186\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproject\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    187\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mproject\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\google\\auth\\_default.py\u001b[0m in \u001b[0;36mdefault\u001b[1;34m(scopes, request)\u001b[0m\n\u001b[0;32m    319\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mcredentials\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meffective_project_id\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 321\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDefaultCredentialsError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_HELP_MESSAGE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mDefaultCredentialsError\u001b[0m: Could not automatically determine credentials. Please set GOOGLE_APPLICATION_CREDENTIALS or explicitly create credentials and re-run the application. For more information, please see https://cloud.google.com/docs/authentication/getting-started"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "client = bigquery.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a query.\n",
    "QUERY = (\n",
    "    'SELECT name FROM `bigquery-public-data.usa_names.usa_1910_2013` '\n",
    "    'WHERE state = \"TX\" '\n",
    "    'LIMIT 100')\n",
    "query_job = client.query(QUERY)  # API request\n",
    "rows = query_job.result()  # Waits for query to finish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZmRszYj6dHXa"
   },
   "source": [
    "2. Write code using a single SQL query that will return a dataframe with the ID of the sample, 3 associated input features and one target label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gAL_o2UeFJNj"
   },
   "outputs": [],
   "source": [
    "# your input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bnuPY0ecgmkP"
   },
   "source": [
    "3. Create a pandas dataframe with all the input features, and a few target labels, cleaned and scaled. For all the question below, make sure the associated inputs and targets are clean, i.e., have no NaN, and are appropriately scaled. Bonus marks if you clean the data using SQL queries only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1tgTCeKaF28G"
   },
   "outputs": [],
   "source": [
    "# your input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ufsCDRZ0hDuk"
   },
   "source": [
    "Now you will use k-NN (for regression) and linear regression model to make predictions on your data set. \n",
    "\n",
    "4. Choose a target label (say `tar_2`). Use all feature inputs (`inp_1` to `inp_20`) to predict `tar_2` with linear regression. Examine if you need regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "avWKc0c7-OVj"
   },
   "outputs": [],
   "source": [
    "# your input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ygb2gn8I-qcT"
   },
   "source": [
    "5. Find the input (`inp_x`) feature that has no impact (or minimum impact) and the most important one in the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W7YaAIzo-thD"
   },
   "outputs": [],
   "source": [
    "# your input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4yID8h3d-PJb"
   },
   "source": [
    "6. Use all inputs and do a PCA on the input data. Then use the first 3 components to predict `tar_2`. Compare the results with linear regression with regularization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DhYX6GOu_AYq"
   },
   "outputs": [],
   "source": [
    "# your input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iGQ0fL1i_BKV"
   },
   "source": [
    "7. Repeat the same procedure for tar_7 and tar_12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gtfOXzcM_dAj"
   },
   "outputs": [],
   "source": [
    "# your input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Is7HgHFChjnU"
   },
   "source": [
    "8. Write code to save your results in a relational database (SQLite or BigQuery)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gd3fE1Ss_hsT"
   },
   "outputs": [],
   "source": [
    "# your input"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "PHYS-555-Assignment5",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
